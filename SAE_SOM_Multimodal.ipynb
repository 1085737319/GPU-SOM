{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAE-SOM-Multimodal",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1NrVJocCOocTMUrGqDWWymFGI6oFLUhBS",
      "authorship_tag": "ABX9TyM16XKTEIrc+TwhCgDmKXSA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q8vsNacAqMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "9c00a306-5814-4cf6-dec4-d6fddee1e020"
      },
      "source": [
        "# import and check for TensorFlow version then install 2.0.0 if not installed\n",
        "\"\"\"\n",
        "!pip install --upgrade tensorflow-gpu\n",
        "\"\"\"\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "#tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard\n",
        "\n",
        "# install scikit-optimize\n",
        "!pip install scikit-optimize\n",
        "\n",
        "# install autotime to get execution time for each cell\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime\n",
        "\n",
        "# install PyDrive\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (0.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.16.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (20.4.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.6/dist-packages (0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYt8s2ME4lb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "72ecce7c-ad98-45a2-ef47-e87bb3303c75"
      },
      "source": [
        "print(tf.__version__)\n",
        "if tf.executing_eagerly():\n",
        "    print(\"Eager execution!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "Eager execution!\n",
            "time: 2.44 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM250EM_tPp_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "555d8bde-1658-4435-e01f-721537926d7b"
      },
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random as ran\n",
        "import skopt\n",
        "import os\n",
        "from skopt import gp_minimize\n",
        "from tensorflow.python.client import timeline\n",
        "from datetime import datetime\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from tensorflow.keras.models import Sequential, load_model, Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Add, Concatenate, Reshape, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, AveragePooling1D, AveragePooling2D, Conv1DTranspose, Conv2DTranspose, UpSampling1D, UpSampling2D\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 59 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6LtRuZzoclm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "58bc2b1a-d9a0-4748-803b-f54d6c7fda3f"
      },
      "source": [
        "# seed\n",
        "seed_value = 23\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "ran.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
        "tf.random.set_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "\"\"\"\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\\nK.set_session(sess)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "text": [
            "time: 13.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyvogwmJtZV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4b3d8510-d83e-4f78-9494-c1bfc7c365f3"
      },
      "source": [
        "# install gpu utilities\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "# import packages\n",
        "import os, sys, humanize, psutil, GPUtil\n",
        "\n",
        "# GPU name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError(\"GPU device not found!\")\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# define function\n",
        "def gpu_report():\n",
        "    print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n",
        "    GPUs = GPUtil.getGPUs()\n",
        "    print(GPUs)\n",
        "    for i, gpu in enumerate(GPUs):\n",
        "        print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
        "\n",
        "# GPU memory check\n",
        "gpu_report()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Found GPU at: /device:GPU:0\n",
            "CPU RAM Free: 25.7 GB\n",
            "[<GPUtil.GPUtil.GPU object at 0x7f3ab94e4cc0>]\n",
            "GPU 0 ... Mem Free: 15921MB / 16280MB | Utilization   2%\n",
            "time: 8.52 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_gGHHkSsdZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46744633-8763-4745-a15f-5781133140a4"
      },
      "source": [
        "# fuction to load data\n",
        "def load_data(link, name):\n",
        "    fluff, id = link.split('d/')\n",
        "    id, _ = id.split('/view')\n",
        "    print(\"--- loading data: \" + name + \" from id -> \" + id + \" ---\")\n",
        "    downloaded = drive.CreateFile({\"id\":id})\n",
        "    downloaded.GetContentFile(name)\n",
        "    data = np.load(name, allow_pickle=True)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.29 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNXXyLHEqsON",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1adcddb5-b4f3-467b-de32-1bef8c990ce8"
      },
      "source": [
        "# links for data loading\n",
        "link_data_wr_train = \"https://drive.google.com/file/d/14P-Kje0uorg7vf_4_Mh2445nAodJufMn/view?usp=sharing\"\n",
        "link_data_sp_train = \"https://drive.google.com/file/d/1mRBFj4eX-shxWsbjMGEQs5BOnymJB14B/view?usp=sharing\"\n",
        "link_labels_dgt_train = \"https://drive.google.com/file/d/1ObYhK-6O22QkLnyPCMjuh4STDfIhuf5K/view?usp=sharing\"\n",
        "link_data_wr_test = \"https://drive.google.com/file/d/1Yzh8kO5Fh9iz2-xNOvIZ4GLdVoge4Tcb/view?usp=sharing\"\n",
        "link_data_sp_test = \"https://drive.google.com/file/d/1CC1EzMNCnwWF52oh7DFTGgxmmbEfHsav/view?usp=sharing\"\n",
        "link_labels_dgt_test = \"https://drive.google.com/file/d/1ZlDG5iDSi6ebamwwYp9tjaEgcghJUqBN/view?usp=sharing\"\n",
        "\n",
        "link_data_dvs_train = \"https://drive.google.com/file/d/1VHsmJAleqP9pp9Nty5ZP9DkNkOFRhyPf/view?usp=sharing\"\n",
        "link_data_emg_train = \"https://drive.google.com/file/d/1-qHwORnLzr4-DJu84kx6D6LfWGnR7Ipm/view?usp=sharing\"\n",
        "link_labels_hnd_train = \"https://drive.google.com/file/d/1PkSYnCUubNp2lbm5aL7Xcwl7UGW1Zj8l/view?usp=sharing\"\n",
        "link_data_dvs_test = \"https://drive.google.com/file/d/1sSMX49D9GwT1isF31i8JJlD34PF7b3SA/view?usp=sharing\"\n",
        "link_data_emg_test = \"https://drive.google.com/file/d/1EnmIMvGTLYeiS5aUCypsSXTKoPMWXJhc/view?usp=sharing\"\n",
        "link_labels_hnd_test = \"https://drive.google.com/file/d/1KLr2KbX5GRltpKVf5LKh6Zshn3nietZG/view?usp=sharing\"\n",
        "\n",
        "\n",
        "# names for data loading\n",
        "name_data_wr_train = \"data_wr_train.npy\"\n",
        "name_data_sp_train = \"data_sp_train.npy\"\n",
        "name_labels_dgt_train = \"labels_dgt_train.npy\"\n",
        "name_data_wr_test = \"data_wr_test.npy\"\n",
        "name_data_sp_test = \"data_sp_test.npy\"\n",
        "name_labels_dgt_test = \"labels_dgt_test.npy\"\n",
        "\n",
        "name_data_dvs_train = \"data_dvs_train.npy\"\n",
        "name_data_emg_train = \"data_emg_train.npy\"\n",
        "name_labels_hnd_train = \"labels_hnd_train.npy\"\n",
        "name_data_dvs_test = \"data_dvs_test.npy\"\n",
        "name_data_emg_test = \"data_emg_test.npy\"\n",
        "name_labels_hnd_test = \"labels_hnd_test.npy\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 12.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeLzgRVFperd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyH3qTTCb5AY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "07728edd-24e1-4b22-843d-3c2581db57b5"
      },
      "source": [
        "# import dataset\n",
        "\"\"\"\n",
        "1 = written digits\n",
        "2 = spoken digits\n",
        "3 = dvs hand gestures\n",
        "4 = emg hand gestures\n",
        "\"\"\"\n",
        "dataset = 4\n",
        "\n",
        "if dataset == 1:\n",
        "    class_nbr = 10\n",
        "    train_data = 60000\n",
        "    label_data = 6000\n",
        "    test_data = 10000\n",
        "    x_train = load_data(link_data_wr_train, name_data_wr_train)\n",
        "    x_train = x_train.reshape((60000, 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
        "    index_train = load_data(link_labels_dgt_train, name_labels_dgt_train)\n",
        "    x_test = load_data(link_data_wr_test, name_data_wr_test)\n",
        "    x_test = x_test.reshape((10000, 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
        "    index_test = load_data(link_labels_dgt_test, name_labels_dgt_test)\n",
        "\n",
        "if dataset == 2:\n",
        "    class_nbr = 10\n",
        "    train_data = 60000\n",
        "    label_data = 6000\n",
        "    test_data = 10000\n",
        "    x_train = load_data(link_data_sp_train, name_data_sp_train)\n",
        "    x_train = x_train.reshape((60000, 39, 13, 1))  # adapt this if using `channels_first` image data format\n",
        "    index_train = load_data(link_labels_dgt_train, name_labels_dgt_train)\n",
        "    x_test = load_data(link_data_sp_test, name_data_sp_test)\n",
        "    x_test = x_test.reshape((10000, 39, 13, 1))  # adapt this if using `channels_first` image data format\n",
        "    index_test = load_data(link_labels_dgt_test, name_labels_dgt_test)\n",
        "\n",
        "if dataset == 3:\n",
        "    class_nbr = 5\n",
        "    train_data = 5400\n",
        "    label_data = 540\n",
        "    test_data = 1350\n",
        "    x_train = load_data(link_data_dvs_train, name_data_dvs_train)\n",
        "    x_train = x_train.reshape((5400, 60, 60, 1))  # adapt this if using `channels_first` image data format\n",
        "    index_train = load_data(link_labels_hnd_train, name_labels_hnd_train)\n",
        "    x_test = load_data(link_data_dvs_test, name_data_dvs_test)\n",
        "    x_test = x_test.reshape((1350, 60, 60, 1))  # adapt this if using `channels_first` image data format\n",
        "    index_test = load_data(link_labels_hnd_test, name_labels_hnd_test)\n",
        "\n",
        "if dataset == 4:\n",
        "    class_nbr = 5\n",
        "    train_data = 5400\n",
        "    label_data = 540\n",
        "    test_data = 1350\n",
        "    x_train = load_data(link_data_emg_train, name_data_emg_train)\n",
        "    x_train = x_train.reshape((5400, 16, 1))  # adapt this if using `channels_first` image data format\n",
        "    index_train = load_data(link_labels_hnd_train, name_labels_hnd_train)\n",
        "    x_test = load_data(link_data_emg_test, name_data_emg_test)\n",
        "    x_test = x_test.reshape((1350, 16, 1))  # adapt this if using `channels_first` image data format\n",
        "    index_test = load_data(link_labels_hnd_test, name_labels_hnd_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- loading data: data_emg_train.npy from id -> 1-qHwORnLzr4-DJu84kx6D6LfWGnR7Ipm ---\n",
            "--- loading data: labels_hnd_train.npy from id -> 1PkSYnCUubNp2lbm5aL7Xcwl7UGW1Zj8l ---\n",
            "--- loading data: data_emg_test.npy from id -> 1EnmIMvGTLYeiS5aUCypsSXTKoPMWXJhc ---\n",
            "--- loading data: labels_hnd_test.npy from id -> 1KLr2KbX5GRltpKVf5LKh6Zshn3nietZG ---\n",
            "time: 5.29 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhhvMcL2thZX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38be98ba-5be5-4186-ebd7-911f0c8d6dcc"
      },
      "source": [
        "class KSOM():\n",
        "    def __init__(self, m, n, dim):\n",
        "        self.m = m\n",
        "        self.n = n\n",
        "        self.dim = dim\n",
        "        self.map_wgt =  tf.Variable(\n",
        "                            tf.random.uniform(\n",
        "                                shape = [m*n, dim],\n",
        "                                minval = 0.0,\n",
        "                                maxval = 1.0,\n",
        "                                dtype = tf.float32,\n",
        "                                seed = 23\n",
        "                            )\n",
        "                        )\n",
        "        self.map_loc =  tf.constant(\n",
        "                            np.array(\n",
        "                                list(self.neuron_locs(m, n))\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "    def neuron_locs(self, m, n):\n",
        "        # nested iterations over both dimensions to yield one by one the 2-d locations of the individual neurons in the SOM\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                yield np.array([i,j], dtype=np.float32)\n",
        "    \n",
        "    def compute_winner(self, sample):\n",
        "            self.sample = sample\n",
        "\n",
        "            # compute the squared euclidean distance between the input and the neurons\n",
        "            self.squared_distance = tf.reduce_sum(\n",
        "                                        tf.square(\n",
        "                                            tf.subtract(\n",
        "                                                self.map_wgt, # [m*n, dim]\n",
        "                                                tf.expand_dims(\n",
        "                                                    self.sample, # [dim] -> [1, dim]\n",
        "                                                    axis=0\n",
        "                                                )\n",
        "                                            )\n",
        "                                        ), \n",
        "                                        axis=1\n",
        "                                    )\n",
        "\n",
        "            # compute the consine distance between the input and the neurons\n",
        "            \"\"\"\n",
        "            self.squared_distance =  tf.subtract(\n",
        "                                          1.0,\n",
        "                                          tf.math.divide(\n",
        "                                              tf.reduce_sum(\n",
        "                                                  tf.multiply(\n",
        "                                                      self.map_wgt, # [m*n, dim]\n",
        "                                                      tf.expand_dims(\n",
        "                                                          self.sample, # [dim] -> [1, dim]\n",
        "                                                          axis=0\n",
        "                                                      )\n",
        "                                                  ),\n",
        "                                                  axis=1\n",
        "                                              ),\n",
        "                                              tf.multiply(\n",
        "                                                  tf.math.sqrt(\n",
        "                                                      tf.reduce_sum(\n",
        "                                                          tf.multiply(\n",
        "                                                              self.map_wgt, # [m*n, dim]\n",
        "                                                              self.map_wgt, # [m*n, dim]\n",
        "                                                          ),\n",
        "                                                          axis=1\n",
        "                                                      )\n",
        "                                                  ),\n",
        "                                                  tf.math.sqrt(\n",
        "                                                      tf.reduce_sum(\n",
        "                                                          tf.multiply(\n",
        "                                                              self.sample, # [dim]\n",
        "                                                              self.sample, # [dim]\n",
        "                                                          ),\n",
        "                                                          axis=0\n",
        "                                                      )\n",
        "                                                  )\n",
        "                                              )\n",
        "                                          )\n",
        "                                      )\n",
        "            \"\"\"\n",
        "            \n",
        "            # find the bmu's index\n",
        "            self.bmu_idx =  tf.argmin(\n",
        "                                    input=self.squared_distance, \n",
        "                                    axis=0\n",
        "                                )\n",
        "            \n",
        "            # extract the bmu's 2-d location\n",
        "            self.bmu_loc =  tf.gather(\n",
        "                                self.map_loc, \n",
        "                                self.bmu_idx\n",
        "                            )\n",
        "    \n",
        "    def update_network(self, epsilon, eta):\n",
        "        # compute the squared manhattan distance between the bmu and the neurons\n",
        "        self.bmu_distance_squares = tf.reduce_sum(\n",
        "                                        tf.square(\n",
        "                                            tf.subtract(\n",
        "                                                self.map_loc, # [m*n, 2]\n",
        "                                                tf.expand_dims(\n",
        "                                                    self.bmu_loc, # [2] -> [1, 2]\n",
        "                                                    axis=0\n",
        "                                                )\n",
        "                                            )\n",
        "                                        ), \n",
        "                                        axis=1\n",
        "                                    )\n",
        "\n",
        "        # compute the neighborhood function\n",
        "        self.neighbourhood_func = tf.exp(\n",
        "                                      tf.negative(\n",
        "                                          tf.math.divide(\n",
        "                                              self.bmu_distance_squares,\n",
        "                                              tf.multiply(\n",
        "                                                  tf.square(\n",
        "                                                      eta,\n",
        "                                                  ),\n",
        "                                                  2.0\n",
        "                                              )\n",
        "                                          )\n",
        "                                      )\n",
        "                                  )\n",
        "\n",
        "        # compute the overall learning of each neuron\n",
        "        self.learning = tf.multiply(\n",
        "                            self.neighbourhood_func, \n",
        "                            epsilon\n",
        "                        )\n",
        "        \n",
        "        # compute the difference between the neurons weights and the input\n",
        "        self.delta_wgt =  tf.subtract(\n",
        "                              tf.expand_dims(\n",
        "                                  self.sample, # [dim] -> [1, dim]\n",
        "                                  axis=0\n",
        "                              ),\n",
        "                              self.map_wgt, # [m*n, dim]\n",
        "                          )\n",
        "\n",
        "        # compute the weights update according to the learning and delta_wgt and update the weights\n",
        "        tf.compat.v1.assign_add(\n",
        "            self.map_wgt,\n",
        "            tf.multiply(\n",
        "                tf.expand_dims(\n",
        "                    self.learning, # [m*n] -> [m*n, 1]\n",
        "                    axis=-1\n",
        "                ),\n",
        "                self.delta_wgt # [m*n, dim]\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    def get_weights(self):\n",
        "        return self.map_wgt\n",
        "\n",
        "    @tf.function\n",
        "    def train(self, nbr_epochs, epsilon_i, epsilon_f, eta_i, eta_f, x_train):\n",
        "        with tf.device('/device:gpu:0'):\n",
        "            for epoch in tf.range(nbr_epochs):\n",
        "                tf.print(\"---------- epoch\", epoch + 1, \"----------\")\n",
        "\n",
        "                # update the learning rate epsilon\n",
        "                epsilon_t =  tf.multiply(\n",
        "                                    epsilon_i,\n",
        "                                    tf.pow(\n",
        "                                        tf.math.divide(\n",
        "                                            epsilon_f, \n",
        "                                            epsilon_i\n",
        "                                        ),\n",
        "                                        tf.cast(\n",
        "                                            tf.math.divide(\n",
        "                                                epoch,\n",
        "                                                nbr_epochs - 1\n",
        "                                            ), \n",
        "                                            dtype=tf.float32\n",
        "                                        )\n",
        "                                    )\n",
        "                                )\n",
        "                \n",
        "                # update the gaussian neighborhood witdh eta\n",
        "                eta_t =  tf.multiply(\n",
        "                                  eta_i, \n",
        "                                  tf.pow(\n",
        "                                      tf.math.divide(\n",
        "                                          eta_f, \n",
        "                                          eta_i\n",
        "                                      ),\n",
        "                                      tf.cast(\n",
        "                                          tf.math.divide(\n",
        "                                              epoch,\n",
        "                                              nbr_epochs - 1\n",
        "                                          ), \n",
        "                                          dtype=tf.float32\n",
        "                                      )\n",
        "                                  )\n",
        "                              )\n",
        "                \n",
        "                # shuffle the training dataset\n",
        "                tf.random.shuffle(x_train)\n",
        "\n",
        "                # bmu computing and network update for each sample\n",
        "                for x_trn in x_train:\n",
        "                    sample = tf.cast(x_trn, dtype=tf.float32)\n",
        "                    self.compute_winner(sample)\n",
        "                    self.update_network(epsilon_t, eta_t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 92.3 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cgcB0ynZgAN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21b32968-4bae-4c76-a82b-26e71fbbfca7"
      },
      "source": [
        "def euclid_dist(x, y):\n",
        "    d = np.linalg.norm(x - y, axis=1)\n",
        "    return d\n",
        "\n",
        "def cosine_dist(x, y):\n",
        "    cosine_similarity =  np.divide(\n",
        "                              np.sum(\n",
        "                                  np.multiply(x, \n",
        "                                              y,\n",
        "                                  ),\n",
        "                                  axis=1\n",
        "                              ),\n",
        "                              np.multiply(\n",
        "                                  np.sqrt(\n",
        "                                      np.sum(\n",
        "                                          np.multiply(x, \n",
        "                                                      x,\n",
        "                                          ),\n",
        "                                          axis=0\n",
        "                                      )\n",
        "                                  ),\n",
        "                                  np.sqrt(\n",
        "                                      np.sum(\n",
        "                                          np.multiply(y, \n",
        "                                                      y\n",
        "                                          ),\n",
        "                                          axis=1\n",
        "                                      )\n",
        "                                  )\n",
        "                              )\n",
        "                          )\n",
        "    \n",
        "    d = 1.0 - cosine_similarity\n",
        "    return d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 8.15 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaxVYcYytkvE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8442617-e198-4a1c-a950-5b1e1be2c487"
      },
      "source": [
        "def labeling(label_data, class_nbr, weights, x_label, index_label, sigma):    \n",
        "    dist_sum = np.zeros((len(weights), class_nbr))\n",
        "    nbr_digits = np.zeros((class_nbr,))\n",
        "\n",
        "    # accumulate the normalized gaussian distance for the labeling dataset\n",
        "    for (x, y) in zip(x_label, index_label):\n",
        "        nbr_digits[y] += 1\n",
        "        dist_neuron = np.exp(-euclid_dist(x, weights)/sigma)\n",
        "        dist_bmu = np.max(dist_neuron)\n",
        "        for i, distn in enumerate(dist_neuron):\n",
        "            dist_sum[i][y] += distn/dist_bmu\n",
        "\n",
        "    # normalize the activities on the number of samples per class\n",
        "    for i, dists in enumerate(dist_sum):\n",
        "        dist_sum[i] = dists/nbr_digits\n",
        "\n",
        "    # assign the neurons labels\n",
        "    neuron_label = np.argmax(dist_sum, axis=1)\n",
        "    #print(\"neurons labels = \")\n",
        "    #print(neuron_label)\n",
        "\n",
        "    return neuron_label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 10.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-rMfPK_tmpv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8119c817-0cba-4377-feaa-4986ff804e08"
      },
      "source": [
        "def test(class_nbr, weights, x_test, index_test, neuron_label, sigma):\n",
        "    neuron_index = np.zeros((len(x_test),), dtype=int)\n",
        "\n",
        "    # calculate the BMUs for the test dataset\n",
        "    for i, x in enumerate(x_test):\n",
        "        dist_neuron = euclid_dist(x, weights)\n",
        "        neuron_index[i] = np.argmin(dist_neuron)\n",
        "    \n",
        "    # compare the BMUs labels and the samples labels\n",
        "    accuracy = 0\n",
        "    for p, t in zip(neuron_index, index_test):\n",
        "        if neuron_label[p] == t:\n",
        "            accuracy += 1\n",
        "    accuracy = (float(accuracy)/len(x_test))*100\n",
        "    print(\"mnist test accuracy = %.2f\" % accuracy)\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 6.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Puz98k59tqqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c6260c4-54d1-409a-9371-b472787b5347"
      },
      "source": [
        "def run_som(eps_i, eps_f, eta_i, eta_f, x_tr, x_lb, index_lb, x_ts, index_ts):\n",
        "    print(\"\\nhyper-parameters:   # eps_i = %f   # eps_f = %f   # eta_i = %f   # eta_f = %f\" % (eps_i, eps_f, eta_i, eta_f))\n",
        "    # train the network\n",
        "    som = KSOM(map_wth, map_hgt, input_dim)\n",
        "    som.train(nbr_epochs, eps_i, eps_f, eta_i, eta_f,x_tr)\n",
        "    weights = som.get_weights().numpy()\n",
        "    \n",
        "    # label the network\n",
        "    neuron_label = labeling(label_data, class_nbr, weights, x_lb, index_lb, sigma_kernel)\n",
        "\n",
        "    # test the network\n",
        "    accuracy = test(class_nbr, weights, x_ts, index_ts, neuron_label, sigma_kernel)\n",
        "\n",
        "    return weights, accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.85 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9-afxl8A6XX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17c34ccb-9a30-44a9-b847-46f7db109f1b"
      },
      "source": [
        "# function to shuffle in unison\n",
        "def unison_shuffle(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "def run_labeling(weights, class_nbr, label_data, x_tr, index_tr, x_ts, index_ts):\n",
        "    for i in range(10):\n",
        "        x_tr, index_tr = unison_shuffle(x_tr, index_tr)\n",
        "        x_lb = np.copy(x_tr[:label_data,:])\n",
        "        index_lb = np.copy(index_tr[:label_data])\n",
        "        # label the network\n",
        "        neuron_label = labeling(label_data, class_nbr, weights, x_lb, index_lb, sigma_kernel)\n",
        "        \n",
        "        # test the network\n",
        "        accuracy = test(class_nbr, weights, x_ts, index_ts, neuron_label, sigma_kernel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.42 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkVMswPmT5qV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a8daab9-1c96-405c-8745-4ba9cd3f75a8"
      },
      "source": [
        "# function to normalize and standardize data\n",
        "def norm_stand(data, data_mean, data_std, data_max, data_min):\n",
        "    print(\"--- normalize and standardize data ---\")\n",
        "    \n",
        "    data = data.astype(float)\n",
        "\n",
        "    # standardize\n",
        "    \"\"\"\n",
        "    if data_mean == None:\n",
        "        data_mean = np.mean(data)\n",
        "    if data_std == None:\n",
        "        data_std = np.std(data) + 1e-15\n",
        "    print(\"data_mean = \", data_mean)\n",
        "    print(\"data_std = \", data_std)\n",
        "    data -= data_mean\n",
        "    data /= data_std\n",
        "    \"\"\"\n",
        "    \n",
        "    # normalize\n",
        "    if data_max == None:\n",
        "        data_max = np.max(data)\n",
        "    if data_min == None:\n",
        "        data_min = np.min(data) \n",
        "    print(\"data_max = \", data_max)\n",
        "    print(\"data_min = \", data_min)\n",
        "    for i in range(len(data)):\n",
        "        data[i] = (data[i] - data_min) / (data_max - data_min)  \n",
        "    \n",
        "    print(\"---\")\n",
        "    print(\"new_data_max = \", np.max(data))\n",
        "    print(\"new_data_min = \", np.min(data))\n",
        "    print(\"new_data_mean = \", np.mean(data))\n",
        "    print(\"new_data_std = \", np.std(data))\n",
        "    print(\"\\n\")\n",
        "    \n",
        "    return data, data_mean, data_std, data_max, data_min"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 12.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdnu07D7H1qo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7988fef8-46ad-4465-90e0-959ff8efb245"
      },
      "source": [
        "kl_divergence = tf.keras.losses.kullback_leibler_divergence # loss = y_true * log(y_true / y_pred)\n",
        "\n",
        "class KLDivergenceRegularizer(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, weight=0.01, target=0.05):\n",
        "        self.weight = weight\n",
        "        self.target = target\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        mean_activities = K.mean(inputs, axis=0)\n",
        "        loss = self.weight * K.sum(\n",
        "            kl_divergence(self.target, mean_activities) +\n",
        "            kl_divergence(1. - self.target, 1. - mean_activities)\n",
        "        )\n",
        "        return loss\n",
        "\n",
        "kld_reg = KLDivergenceRegularizer(weight=1e-4, target=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.74 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnMpP9COws4D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f2aab4d-b67d-4ce8-eca0-0d77297bbaea"
      },
      "source": [
        "def weight_bound(fan_in, fan_out):\n",
        "    bound = np.sqrt(\n",
        "                np.divide(\n",
        "                    6.0,\n",
        "                    np.add(\n",
        "                        fan_in,\n",
        "                        fan_out\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "    return bound"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.33 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usDN98xwIidx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ee618bc9-32e1-4051-ea8b-0b5d25b5c7cf"
      },
      "source": [
        "# standardize/normalize data\n",
        "x_train, data_mean, data_std, data_max, data_min = norm_stand(x_train, None, None, None, None)\n",
        "x_test, data_mean, data_std, data_max, data_min = norm_stand(x_test, data_mean, data_std, data_max, data_min)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- normalize and standardize data ---\n",
            "data_max =  7.334081930525158\n",
            "data_min =  -1.2475608376565506\n",
            "---\n",
            "new_data_max =  1.0\n",
            "new_data_min =  0.0\n",
            "new_data_mean =  0.14489273815932827\n",
            "new_data_std =  0.11593214487955045\n",
            "\n",
            "\n",
            "--- normalize and standardize data ---\n",
            "data_max =  7.334081930525158\n",
            "data_min =  -1.2475608376565506\n",
            "---\n",
            "new_data_max =  0.9472715918106641\n",
            "new_data_min =  0.0\n",
            "new_data_mean =  0.1473066810263437\n",
            "new_data_std =  0.11886099780142306\n",
            "\n",
            "\n",
            "time: 36.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TbC5DQjH-BD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "9aa5033d-c2da-44b8-ff9d-000e31eb2236"
      },
      "source": [
        "# Custom topology with the best performance\n",
        "# Accuracy on MNIST: 96.83 - 96.53 - 96.97\n",
        "\"\"\"\n",
        "ker_reg = 1e-4\n",
        "act_reg = 1e-4 \n",
        "features_layer = 3\n",
        "input_dim = 4096\n",
        "\n",
        "input_img = Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format\n",
        "e = Conv2D(64, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(input_img)\n",
        "e = Conv2D(256, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(ker_reg), activity_regularizer=regularizers.l1(act_reg))(e)\n",
        "\n",
        "encoded = MaxPooling2D((5, 5))(e) # at this point the representation is (4, 4, 256) i.e. 4096-dimensional\n",
        "\n",
        "d = UpSampling2D((5, 5))(encoded)\n",
        "d = Conv2DTranspose(64, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(d)\n",
        "decoded = Conv2DTranspose(1, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(d)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nker_reg = 1e-4\\nact_reg = 1e-4 \\nfeatures_layer = 3\\ninput_dim = 4096\\n\\ninput_img = Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format\\ne = Conv2D(64, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(input_img)\\ne = Conv2D(256, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(ker_reg), activity_regularizer=regularizers.l1(act_reg))(e)\\n\\nencoded = MaxPooling2D((5, 5))(e) # at this point the representation is (4, 4, 256) i.e. 4096-dimensional\\n\\nd = UpSampling2D((5, 5))(encoded)\\nd = Conv2DTranspose(64, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(d)\\ndecoded = Conv2DTranspose(1, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(d)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "stream",
          "text": [
            "time: 3.52 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dPWRP8u5-Rk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b88d46a4-11a4-4734-bb53-e621612fe478"
      },
      "source": [
        "# Network to use\n",
        "# Custom topology with the best performance\n",
        "ker_reg = 1e-4\n",
        "act_reg = 1e-4 \n",
        "\n",
        "if dataset == 1: \n",
        "    # Accuracy: 96\n",
        "    features_layer = 3\n",
        "    input_dim = 4096\n",
        "    \n",
        "    input_img = Input(shape=(28, 28, 1))\n",
        "    e = Conv2D(64, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(input_img)\n",
        "    e = Conv2D(256, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(ker_reg), activity_regularizer=regularizers.l1(act_reg))(e)\n",
        "\n",
        "    encoded = MaxPooling2D((5, 5))(e) # at this point the representation is (4, 4, 256) i.e. 4096-dimensional\n",
        "\n",
        "    d = UpSampling2D((5, 5))(encoded)\n",
        "    d = Conv2DTranspose(64, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(d)\n",
        "    decoded = Conv2DTranspose(1, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(d)\n",
        "\n",
        "elif dataset == 2:\n",
        "    # Accuracy: 50\n",
        "    features_layer = 3\n",
        "    input_dim = 4608\n",
        "    \n",
        "    input_img = Input(shape=(39, 13, 1))\n",
        "    e = Conv2D(64, (6, 3), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(input_img)\n",
        "    e = Conv2D(256, (5, 3), activation='relu', kernel_regularizer=regularizers.l2(ker_reg), activity_regularizer=regularizers.l1(act_reg))(e)\n",
        "\n",
        "    encoded = MaxPooling2D((5, 3))(e) # at this point the representation is (4, 4, 256) i.e. 4096-dimensional\n",
        "\n",
        "    d = UpSampling2D((5, 3))(encoded)\n",
        "    d = Conv2DTranspose(64, (5, 3), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(d)\n",
        "    decoded = Conv2DTranspose(1, (6, 3), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(d)\n",
        "\n",
        "elif dataset == 3:\n",
        "    # Accuracy: 45\n",
        "    features_layer = 3\n",
        "    input_dim = 12544\n",
        "    \n",
        "    input_img = Input(shape=(60, 60, 1))\n",
        "    e = Conv2D(64, (7, 7), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(input_img)\n",
        "    e = Conv2D(256, (6, 6), activation='relu', kernel_regularizer=regularizers.l2(ker_reg), activity_regularizer=regularizers.l1(act_reg))(e)\n",
        "\n",
        "    encoded = MaxPooling2D((7, 7))(e) # at this point the representation is (4, 4, 256) i.e. 4096-dimensional\n",
        "\n",
        "    d = UpSampling2D((7, 7))(encoded)\n",
        "    d = Conv2DTranspose(64, (6, 6), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(d)\n",
        "    decoded = Conv2DTranspose(1, (7, 7), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(d)\n",
        "\n",
        "elif dataset == 4:\n",
        "    # Accuracy: 45\n",
        "    features_layer = 3\n",
        "    input_dim = 1024\n",
        "    \n",
        "    input_img = Input(shape=(16, 1))\n",
        "    e = Conv1D(64, (3), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(input_img)\n",
        "    e = Conv1D(256, (3), activation='relu', kernel_regularizer=regularizers.l2(ker_reg), activity_regularizer=regularizers.l1(act_reg))(e)\n",
        "\n",
        "    encoded = MaxPooling1D((3))(e) # at this point the representation is (4, 4, 256) i.e. 4096-dimensional\n",
        "\n",
        "    d = UpSampling1D((3))(encoded)\n",
        "    d = Conv1DTranspose(64, (3), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(d)\n",
        "    decoded = Conv1DTranspose(1, (3), activation='relu', kernel_regularizer=regularizers.l2(ker_reg))(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 165 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r2FJVaLfkYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "a355884c-3ca0-4b5d-d36c-8d3aa269e68f"
      },
      "source": [
        "# create the model\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 16, 1)]           0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 14, 64)            256       \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 12, 256)           49408     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 4, 256)            0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_1 (UpSampling1 (None, 12, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_transpose_2 (Conv1DTr (None, 14, 64)            49216     \n",
            "_________________________________________________________________\n",
            "conv1d_transpose_3 (Conv1DTr (None, 16, 1)             193       \n",
            "=================================================================\n",
            "Total params: 99,073\n",
            "Trainable params: 99,073\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "time: 14.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd9rcvGicNDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8085aafe-2d67-4fbe-de4d-de896bfcf1f4"
      },
      "source": [
        "# compile it\n",
        "# sgd, adam or adadelta?\n",
        "autoencoder.compile(\n",
        "    optimizer=tf.keras.optimizers.Adadelta( \n",
        "        learning_rate=1.0,\n",
        "        rho=0.95,\n",
        "        decay=0,\n",
        "        epsilon=None\n",
        "    ),\n",
        "    loss='mse' # loss = \"binary_crossentropy\" or \"mse\"?\n",
        ")\n",
        "\n",
        "# train it\n",
        "autoencoder.fit(\n",
        "    x_train, x_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    validation_data=(x_test, x_test)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0335 - val_loss: 0.0306\n",
            "Epoch 2/10\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.0292 - val_loss: 0.0289\n",
            "Epoch 3/10\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.0280 - val_loss: 0.0275\n",
            "Epoch 4/10\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0266\n",
            "Epoch 5/10\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0260 - val_loss: 0.0260\n",
            "Epoch 6/10\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.0253 - val_loss: 0.0254\n",
            "Epoch 7/10\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0246 - val_loss: 0.0246\n",
            "Epoch 8/10\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.0240 - val_loss: 0.0241\n",
            "Epoch 9/10\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0234 - val_loss: 0.0234\n",
            "Epoch 10/10\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.0230 - val_loss: 0.0227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3aa8653cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "stream",
          "text": [
            "time: 5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2gpqjt-cao1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "707fe3db-2e2c-49d6-bdb5-e5e6f058f290"
      },
      "source": [
        "# predict train and test datasets\n",
        "pred_train = autoencoder.predict(x_train, verbose=1)\n",
        "pred_test = autoencoder.predict(x_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "169/169 [==============================] - 0s 2ms/step\n",
            "43/43 [==============================] - 0s 2ms/step\n",
            "time: 649 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbh8OfDKk_DM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a829ecb-a924-4ae2-84d6-e085184ab53d"
      },
      "source": [
        "# display decoded image\n",
        "if dataset == 1:\n",
        "    x = pred_test[23].reshape(28,28)\n",
        "    plt.imshow(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.41 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ea4OxntrIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c414d600-1c5b-4672-93e4-5af33345a5d7"
      },
      "source": [
        "def extract_flatten(model, data, layer):\n",
        "    # extracts and flatten the output of the encoder\n",
        "    encoder_out = model.layers[layer].output\n",
        "    encoder_out_flat = Flatten()(encoder_out)\n",
        "\n",
        "    # creates a model that will return these outputs given the model input\n",
        "    model_feat = Model(inputs=model.input, outputs=encoder_out_flat)\n",
        "\n",
        "    # predict the features\n",
        "    features = model_feat.predict(data)\n",
        "\n",
        "    print(\"features.shape = \", features.shape)\n",
        "    \n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.28 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOrURSVgzhXq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6cb706c4-f500-44c8-dd64-ecceae3c1b01"
      },
      "source": [
        "# extract features\n",
        "feat_train = extract_flatten(autoencoder, x_train, features_layer)\n",
        "feat_test = extract_flatten(autoencoder, x_test, features_layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features.shape =  (5400, 1024)\n",
            "features.shape =  (1350, 1024)\n",
            "time: 451 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chRf4FBJTcsH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "746e7309-b6b4-40c0-b18c-2d45e50bce66"
      },
      "source": [
        "# standardize/normalize features\n",
        "feat_train, data_mean, data_std, data_max, data_min = norm_stand(feat_train, None, None, None, None)\n",
        "feat_test, data_mean, data_std, data_max, data_min = norm_stand(feat_test, data_mean, data_std, data_max, data_min)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- normalize and standardize data ---\n",
            "data_max =  0.17580275237560272\n",
            "data_min =  0.0\n",
            "---\n",
            "new_data_max =  1.0\n",
            "new_data_min =  0.0\n",
            "new_data_mean =  0.02806861212294918\n",
            "new_data_std =  0.06514685215945959\n",
            "\n",
            "\n",
            "--- normalize and standardize data ---\n",
            "data_max =  0.17580275237560272\n",
            "data_min =  0.0\n",
            "---\n",
            "new_data_max =  1.007479621624925\n",
            "new_data_min =  0.0\n",
            "new_data_mean =  0.02867590781169114\n",
            "new_data_std =  0.06656897141028191\n",
            "\n",
            "\n",
            "time: 126 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQECShwZ2vbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2d41de7-eccb-4dcf-91bb-d369cafdcaf4"
      },
      "source": [
        "# features dataset\n",
        "x_tr = np.copy(feat_train[:train_data,:])\n",
        "index_tr = np.copy(index_train[:train_data])\n",
        "x_lb = np.copy(feat_train[:label_data,:])\n",
        "index_lb = np.copy(index_train[:label_data])\n",
        "x_ts = np.copy(feat_test[:test_data,:])\n",
        "index_ts = np.copy(index_test[:test_data])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 14.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0edC61wtyIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "342fed12-eb57-4990-bf0f-c8a5024719a0"
      },
      "source": [
        "# hyper-parameters\n",
        "map_wth = 16\n",
        "map_hgt = 16\n",
        "#input_dim = 784\n",
        "#class_nbr = 5\n",
        "nbr_epochs = 10\n",
        "eps_i_list = [1.0]\n",
        "eps_f_list = [0.01]\n",
        "eta_i_list = [10.0]\n",
        "eta_f_list = [0.01]\n",
        "sigma_kernel = 1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.66 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSmY2p3QwYEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f20d4ada-f789-4d14-f4ec-6e8716aea64a"
      },
      "source": [
        "# hyper-parameters grid search\n",
        "hyper_param_list, accuracy_list = [], []\n",
        "for eps_i in eps_i_list:\n",
        "    for eps_f in eps_f_list:\n",
        "        for eta_i in eta_i_list:\n",
        "            for eta_f in eta_f_list:\n",
        "                hyper_param_list.append([eps_i, eps_f, eta_i, eta_f])\n",
        "                weights, accuracy = run_som(eps_i, eps_f, eta_i, eta_f, x_tr, x_lb, index_lb, x_ts, index_ts)\n",
        "                accuracy_list.append(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "hyper-parameters:   # eps_i = 1.000000   # eps_f = 0.010000   # eta_i = 10.000000   # eta_f = 0.010000\n",
            "---------- epoch 1 ----------\n",
            "---------- epoch 2 ----------\n",
            "---------- epoch 3 ----------\n",
            "---------- epoch 4 ----------\n",
            "---------- epoch 5 ----------\n",
            "---------- epoch 6 ----------\n",
            "---------- epoch 7 ----------\n",
            "---------- epoch 8 ----------\n",
            "---------- epoch 9 ----------\n",
            "---------- epoch 10 ----------\n",
            "mnist test accuracy = 44.67\n",
            "time: 44.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxSTTeh3wZrs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "39b94c1e-e79e-4cb1-f986-d153f7bdb11b"
      },
      "source": [
        "# best hyper-parameters\n",
        "best_accuracy = np.max(accuracy_list)\n",
        "best_hyper_param = hyper_param_list[np.argmax(accuracy_list)]\n",
        "print(\"best accuracy = \", best_accuracy)\n",
        "print(\"best hyper-parameters:   # eps_i = %f   # eps_f = %f   # sig_i = %f   # sig_f = %f\" % (best_hyper_param[0], best_hyper_param[1], best_hyper_param[2], best_hyper_param[3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best accuracy =  44.666666666666664\n",
            "best hyper-parameters:   # eps_i = 1.000000   # eps_f = 0.010000   # sig_i = 10.000000   # sig_f = 0.010000\n",
            "time: 4.01 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrZ9v0LXBBZp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f0d0c947-36c7-4473-9451-f34fee053d18"
      },
      "source": [
        "# run labeling\n",
        "label_data = 600\n",
        "run_labeling(weights, class_nbr, label_data, x_tr, index_tr, x_ts, index_ts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mnist test accuracy = 43.56\n",
            "mnist test accuracy = 39.33\n",
            "mnist test accuracy = 40.07\n",
            "mnist test accuracy = 39.33\n",
            "mnist test accuracy = 41.33\n",
            "mnist test accuracy = 39.19\n",
            "mnist test accuracy = 40.89\n",
            "mnist test accuracy = 40.96\n",
            "mnist test accuracy = 39.56\n",
            "mnist test accuracy = 42.96\n",
            "time: 20.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDzEPXk4wbi5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "b25dd291-8075-4be5-d4f8-a85c401d902f"
      },
      "source": [
        "# GPU memory check\n",
        "gpu_report()\n",
        "\n",
        "\"\"\"\n",
        "# display neurons weights as mnist digits\n",
        "som_grid = plt.figure(figsize=(10, 10)) # width, height in inches\n",
        "for n in range(map_wth*map_hgt):\n",
        "    image = weights[n].reshape([80,80]) # x_train[num] is the 784 normalized pixel values\n",
        "    sub = som_grid.add_subplot(map_wth, map_hgt, n + 1)\n",
        "    sub.set_axis_off()\n",
        "    clr = sub.imshow(image, cmap = plt.get_cmap(\"jet\"), interpolation = \"nearest\")\n",
        "    #plt.colorbar(clr)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU RAM Free: 25.0 GB\n",
            "[<GPUtil.GPUtil.GPU object at 0x7f3aa80eac18>]\n",
            "GPU 0 ... Mem Free: 15463MB / 16280MB | Utilization   5%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# display neurons weights as mnist digits\\nsom_grid = plt.figure(figsize=(10, 10)) # width, height in inches\\nfor n in range(map_wth*map_hgt):\\n    image = weights[n].reshape([80,80]) # x_train[num] is the 784 normalized pixel values\\n    sub = som_grid.add_subplot(map_wth, map_hgt, n + 1)\\n    sub.set_axis_off()\\n    clr = sub.imshow(image, cmap = plt.get_cmap(\"jet\"), interpolation = \"nearest\")\\n    #plt.colorbar(clr)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "stream",
          "text": [
            "time: 67.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkePgcmVYFl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}